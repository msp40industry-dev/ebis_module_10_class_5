## Construyendo Workflows de IA con FastAPI y LangGraph

En esta sesi칩n crearemos una aplicaci칩n basada en un LLM utilizando las siguientes herramientas:

- OpenAI: como LLM
- LangGraph: como orquestador de la l칩gica del agente
- FastAPI: (back-end) framework para definir nuestros endpoints
- Streamlit: (front-end) framework para visualizar e interactuar con el agente
- Docker: para meter la aplicaci칩n en un contenedor y poder desplegarla en otros entornos

En esta sesi칩n nos centraremos en entender como encajan las diferentes piezas del proceso, como conectar unas con otras
y cuales la funci칩n de cada una. La l칩gica del agente ser치 una llamada al LLM sencilla, que luego en funci칩n del caso de
uso se puede transformar simplemente modificando el grafo.


A continuaci칩n aparecen los pasos que seguir para la correcta creaci칩n e instalaci칩n de la pr치ctica:

### 1. Setup del proyecto

Entro dentro de la carpeta del proyecto:
```
cd tema10_clase5_practica
```

Creamos un nuevo entorno virtual, con python >= 10. Para esto es libre que cada persona lo haga de la forma que m치s
acostumbrada est치. En mi caso estoy haciendolo con conda:

```
conda create -n <myenv> python=3.10
conda activate <myenv>
```

Una vez lista y activada, instalamos las librerias con el siguiente comando:
```
pip install -r requirements.txt
```

Las librer칤as que necesitaremos se pueden encontrar dentro del requirements.txt.

游 Para configurar el python correctamente en el environment del repositorio, ejecutar el comando `conda info --envs` 
para saber en que localizaci칩n se encuentra nuestro environment.

### 2. Configuraci칩n key de OPENAI

Para la configuraci칩n de la key de openai como variable de entorno, pod칠is seguir este tutorial:
[Best Practices for API Key Safety](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety).


### 3. Ejecutar el backend

Una vez tengamos todo instalado y configurado, podemos lanzar el servidor y comprobar que no hay problemas de
instalaciones ni de c칩didgo. Para ello ejecutar desde la terminal dentro del repositorio:

```
uvicorn backend:app --reload
```

Si se ejecuta correctamente, podremos encontrar nuestro endpoint en: `http://127.0.0.1:8000/docs`.

![img.png](img.png)

Podemos utilizar el siguiente texto para testear que la aplicaci칩n funciona:
```
{
  "user_input": "S칤 porfa!",
  "history": [
    {"role": "user", "content": "Soy programadora y quiero mejorar en Python"},
    {"role": "assistant", "content": "Perfecto, 쯤uieres que te recomiende recursos gratuitos para practicar?"}
  ]
}
```
Otra forma de testear que la API funciona ser칤a lanzando un curl desde terminal:

```
curl -X POST "http://127.0.0.1:8000/chatbot" \
     -H "Content-Type: application/json" \
     -d '{"user_input":"Para que sirve este chatbot?"}'
```

### 4. Ejecutar el frontend

Para ejecutar nuestro frontend, como es un archivo de python sencillo, solo har치 falta
```
python frontend.py
```
Es importante que est칠 lanzado el front, ya que llamaremos a ese endpoint para obtener respueta.

En la terminal aparecer치 lo siguiente:

* Running on local URL:  http://127.0.0.1:7861
* Running on public URL: https://8b89da4077f347e4ff.gradio.live

Las URLs no siempre ser치n las msimas. La URL p칰blica generada, podr치 ser utilizada durante 1 semana por cualquier
persona como si fuera una web.

### 5. Dockerizar

Ya tenemos nuestra aplicaci칩n ejecutada, funcionando y recibiendo requests. Vamos a entender que opciones tenemos para
Dockerizarla y as칤 elegir que nos conviene m치s. En Docker podemos crear un Dockerfile como hicimos en pr치cticas previas,
este quedar칤a de la siguiente forma: 

```
FROM python:3.10

WORKDIR /app
COPY . .

RUN pip install -r requirements.txt

# Comando para ejecutar el backend y el frontend en paralelo
CMD bash -c "python frontend.py & uvicorn backend:app --host 0.0.0.0 --port 8000"
```

En este caso estariamos creando un 칰nico contenedor que tendr칤a dentro frontend y backend. Como punto negativo, si uno
de los dos falla, todo el entorno caer치. 

Por ello, en estas situaciones la mejor pr치ctica ser칤a crear dos contenedores diferentes ya que frontend (Gradio) y
backend (FastAPI) son servicios distintos. Para ello crearemos un `docker-compose.yml` y modificamos nuesto `Dockerfile`
para ejecutar directamente el yml. 

Lanzando:
```
docker compose build
```

se construir치n nuestros contenedores. Para lanzarlo y poder corroborar que todo est치 correctamente construido
ejecutaremos:

```
docker compose up
```
O podemos directamente lanzar: `docker compose up --build` , as칤 de cada vez se construir치 y lanzar치.


----
## Multiagente
En l칤nea con lo comentado en la 칰ltima sesi칩n, a침ad칤 un ejemplo de multiagente para poder revisar como funcionar칤a
una estructura con diferentes partes comunic치ndose entre ellas.

Para lanzar este nuevo agente utilizando el front simplemente tendremos que modificar la url a la que 
apuntamos, ahora el endpoint ser치 el de multiagente. Modificamos en el `frontend.py` la primera l칤nea e incluimos
`http://localhost:8000/chatbot`.

El resto de ejecuciones y pasos a seguir ser치n exactamente iguales.